# üöÄ Big Data Project ‚Äì Airflow + PySpark + DataLake

Pipeline de traitement de donn√©es automatis√© et modulaire utilisant Apache Airflow, PySpark, API KAGGLE, DataLake, Elasticsearch et Kibana.

---

## üìÅ Pr√©requis

- Syst√®me Linux/macOS ou Windows avec WSL
- Python 3.7+
- pip
- git
- Compte Kaggle avec cl√© API (`kaggle.json`)

---

## üß± Installation & Configuration

### üîÅ Passage sous WSL (si Windows)
Suivre le guide officiel : [Installation WSL](https://learn.microsoft.com/fr-fr/windows/wsl/install)

---

## üîß Installation

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/Booboo123478/Big-Data-Project.git
cd Big-Data-Project
```

### 2. Cr√©er un environnement virtuel

```bash
sudo apt update
sudo apt install python3-venv -y
python3 -m venv airflow_venv
source airflow_venv/bin/activate
```

### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

### 4. Installer Airflow

```bash
PYTHON_VERSION=$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)
pip install "apache-airflow[celery]==2.9.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.0/constraints-$PYTHON_VERSION.txt"
```

---

## üîó Configuration Airflow

### 1. Modifier le dossier des DAGs (si besoin)

```bash
export AIRFLOW__CORE__DAGS_FOLDER=$(pwd)/dags
```

Ou modifier dans `~/airflow/airflow.cfg` :

```ini
# dags_folder = /home/username/airflow/dags
dags_folder = /mnt/c/Users/<TON_USER>/Documents/GitHub/Big-Data-Project/dags
```

### 2. Connexion Spark dans Airflow

Interface Airflow > Admin > Connections > Ajouter

- Conn Id: `spark_default`
- Conn Type: `Spark`
- Host: `local`
- Extra:

```json
{
  "master": "local[*]"
}
```

---

## üì¶ KAGGLE

### 1. Mettre `kaggle.json` dans `config/`

Depuis le site Kaggle > Mon compte > Create New Token

### 2. Exporter la variable

```bash
export KAGGLE_CONFIG_DIR=$(pwd)/config
```

---

## ‚ú® Lancement Airflow

### Rendre les scripts ex√©cutables :

```bash
chmod +x start_airflow.sh stop_airflow.sh
```

### Lancer

```bash
cd scripts
./start_airflow.sh
```

Airflow accessible sur [http://localhost:8080](http://localhost:8080)

### Arr√™ter

```bash
cd scripts
./stop_airflow.sh
```

---

## üß© Spark

### 1. Installer Java

```bash
sudo apt update
sudo apt install openjdk-17-jdk -y
```

### 2. V√©rifier le chemin Java

```bash
readlink -f $(which java)
```

Mettre ce chemin dans `start_airflow.sh` sous `JAVA_HOME`

### 3. Installer Spark

```bash
wget https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
tar -xvzf spark-3.4.1-bin-hadoop3.tgz
rm spark-3.4.1-bin-hadoop3.tgz
```

Ajouter au `~/.bashrc` ou temporairement :

```bash
export SPARK_HOME=$(pwd)/spark-3.4.1-bin-hadoop3
export PATH=$PATH:$SPARK_HOME/bin
```

---

## üîç Elasticsearch & Kibana

### 1. T√©l√©charger et copier depuis Windows vers WSL

```bash
cp /mnt/c/Users/<USER>/Downloads/elasticsearch-<ver>.tar.gz .
cp /mnt/c/Users/<USER>/Downloads/kibana-<ver>.tar.gz .
```

### 2. D√©compresser

```bash
tar -xvzf elasticsearch-<ver>.tar.gz
tar -xvzf kibana-<ver>.tar.gz
rm *.tar.gz
```

### 3. Lancer Elasticsearch

```bash
cd elasticsearch-<ver>
bin/elasticsearch
```

### 4. Lancer Kibana

```bash
cd kibana-<ver>
bin/kibana
```

### 5. Suivre les instructions de v√©rification :

- Copier/coller le code d‚Äôenrollment
- V√©rifier le token et code d‚Äôauthentification

---

## üìä Kibana - Dashboard

### Importer un dashboard `.ndjson`

1. Aller dans "Stack Management" > "Saved Objects" > Import
2. S√©lectionner le fichier `.ndjson`
3. Aller dans "Dashboards" > ouvrir

---

## üì§ Exporter vers Windows

Dans le terminal WSL :

```bash
cp -r ~/Big-Data-Project /mnt/c/Users/<TON_USER>/Desktop/
```

---

## üìå Ex√©cution de l‚Äôindexation manuellement

```bash
cd indexing/
python3 indexing_kpi.py
```

# Elasticsearch + Kibana

Je pr√©cise que j'ai tout mis en fonction de mon User (Les premi√®res commandes)

### Exemple avec mon emplacement de DL
C:\Users\marca\Downloads\elasticsearch-9.0.2-linux-x86_64.tar.gz
C:\Users\marca\Downloads\kibana-9.0.2-linux-x86_64.tar.gz

### D√©placer les dossier : Sur WSL ~/Big-Data-Project$ 
```Bash
cp /mnt/c/Users/marca/Downloads/elasticsearch-9.0.2-linux-x86_64.tar.gz ~/Big-Data-Project/
```
```Bash
cp /mnt/c/Users/marca/Downloads/kibana-9.0.2-linux-x86_64.tar.gz ~/Big-Data-Project/
```

### D√©zipper les dossier : Sur WSL ~/Big-Data-Project$
```Bash
tar -xvzf elasticsearch-9.0.2-linux-x86_64.tar.gz
```
```Bash
tar -xvzf kibana-9.0.2-linux-x86_64.tar.gz
```
### Supprimer les dossier zipp√© : Sur WSL ~/Big-Data-Project$
```Bash
rm -rf ~/Big-Data-Project/elasticsearch-9.0.2-linux-x86_64.tar.gz
```
```Bash
rm -rf ~/Big-Data-Project/kibana-9.0.2-linux-x86_64.tar.gz
```

### Lancer Elasticsearch : Sur WSL ~/Big-Data-Project/elasticsearch-9.0.2$ 
Je te conseil d'ouvrir un nouveau terminal
```Bash
bin/elasticsearch
```
Tu re√ßois un MDP, prend le en note (Exemple du miens : R8=vg8aFslx9fySEIpGY) 
Il faudra modifier le MDP dans le fichier d'indexion.

### Lancer Kibana : Sur WSL ~/Big-Data-Project/kibana-9.0.2$
Je te conseil d'ouvrir un nouveau terminal
```Bash
bin/kibana
```

### R√©cup√©ration d eEnrollment token : Sur WSL ~/Big-Data-Project/elasticsearch-9.0.2$
Enrollment token : 
```Bash
bin/elasticsearch-create-enrollment-token -s kibana
```

### Code de v√©rification : Sur WSL ~/Big-Data-Project/kibana-9.0.2$
Copy the code : 
```Bash
bin/kibana-verification-code
```

### Lancer Manuellement L'indexion : Sur WSL ~/Big-Data-Project/indexing$
```Bash
python3 indexing_kpi.py
```

### Sur Kibana
Username : elastic
Password : Ton MDP (Exemple du miens : R8=vg8aFslx9fySEIpGY) 

Barre de recherche : Visualise library

Create Data View
Nom : kpis
index patern : kpis
Save Data View to Kibana

Create New Visualisation
Lens : permet la cr√©ation des √©l√©ments du Dashboard

### Import du Dashboard
üì• 1Ô∏è‚É£ Acc√©der √† la gestion des objets enregistr√©s
Va dans Kibana.

Clique sur "Stack Management".

Dans la section "Saved Objects", trouve l'option "Import".

üîÑ 2Ô∏è‚É£ Importer le fichier JSON
Clique sur "Import".

S√©lectionne ton fichier JSON export√©.

Valide l'importation et assure-toi que tout est bien pris en compte.

üöÄ 3Ô∏è‚É£ V√©rifier le dashboard
Apr√®s l'importation, va dans "Dashboards".

Ton dashboard import√© devrait appara√Ætre.

Ouvre-le et v√©rifie que les donn√©es sont correctes.