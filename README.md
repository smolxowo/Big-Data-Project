# üöÄ Big Data Project ‚Äì Airflow + PySpark + DataLake

Pipeline de traitement de donn√©es automatis√© et modulaire utilisant Apache Airflow, PySpark, API REST et DataLake.

---

## üìÅ Pr√©requis

- Syst√®me Linux, macOS ou Windows avec WSL
- Python 3.7+
- pip
- git
- Compte Kaggle avec cl√© API (`kaggle.json`)

Pour Windows, installer WSL : [Installation WSL](https://learn.microsoft.com/fr-fr/windows/wsl/install)

---

## üß± Installation

### 1. Cloner le d√©p√¥t

`git clone https://github.com/Booboo123478/Big-Data-Project.git`  
`cd Big-Data-Project`

### 2. Installer les d√©pendances du projet (Dans le projet)

`pip install -r requirements.txt`

### 3. Chemin classic (Dans WSL) (√† red√©finir √† votre cas)

`cd /mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project`

### 4. Installer python3-venv (DANS WSL √† faire une fois)

`sudo apt update`  
`sudo apt install python3-venv`

### 5. Cr√©er un environnement virtuel Python (DANS WSL √† faire une fois)

`python3 -m venv airflow_venv`

### 6. Activer un environnement virtuel Python (DANS WSL)

`source airflow_venv/bin/activate`

### 7. Installer Apache Airflow (version 2.9.0 avec Celery) (DANS WSL √† faire une fois)

R√©cup√©rer la version de Python :  
`PYTHON_VERSION=$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)`  
Puis installer Airflow avec la contrainte correspondante :  
`pip install "apache-airflow[celery]==2.9.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.0/constraints-$PYTHON_VERSION.txt"`

### 8. D√©finir le dossier des DAGs (Dans le cas o√π tu as d√©j√† d√©finis des DAGs ailleurs)

`export AIRFLOW__CORE__DAGS_FOLDER=/mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project/dags`

#### Ou

`nano ~/airflow/airflow.cfg`

#### Met en commentaire la ligne suivante

`dags_folder = /home/<TON_USER>/airflow/dags`

#### Et rajoute la suivante

`dags_folder = /mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project/dags`

### 9. Lancer Airflow sur le port 8080

`airflow standalone`

## KAGGLE

### 1. Rentrer cl√© API KAGGLE dans dossier "config"

`Kaggle -> Settings -> API -> Create New Token -> mettre "kaggle.json" dans "config" `

### 2. D√©finir emplacement cl√© API KAGGLE

`export KAGGLE_CONFIG_DIR=/chemin/vers/mon-projet/config`

### 3. T√©l√©charger Data Lake

`python .\download_netflix.py`
`python .\download_tmdb_movies.py`

## Spark

### 1. Installer Java
`sudo apt update`
`sudo apt install openjdk-17-jdk -y`

V√©rification :
`readlink -f $(which java)`

Tu dois obtenir un chemin comme :
`/usr/lib/jvm/java-17-openjdk-amd64/bin/java`

V√©rifie que √ßa correspond bien au script `start_airflow.sh`

---

## ‚öôÔ∏è Utilisation

- L‚Äôinterface Airflow est accessible sur http://localhost:8080
- D√©ployer et g√©rer vos DAGs via l‚Äôinterface
- Le scheduler orchestre les ex√©cutions selon les plannings

## Scripts d√©marrage et arr√™tage Airflow

Rend ex√©cutables les scripts (une seule fois) :
Dans le terminal WSL du projet : `chmod +x start_airflow.sh stop_airflow.sh`

### D√©marrer Airflow

Dans le terminal WSL : `./start_airflow.sh`

Ce script :
- Active automatiquement l‚Äôenvironnement virtuel airflow_venv
- Configure le bon dossier de DAGs pour ce projet (dags/)
- Lance le serveur Airflow (webserver, scheduler, etc.)
Une fois lanc√© :
- Acc√©dez √† l'interface : http://localhost:8080
- Identifiants par d√©faut : admin / admin

### Arr√™ter Airflow proprement

Dans le terminal WSL : `./stop_airflow.sh`

# Elasticsearch + Kibana

Je pr√©cise que j'ai tout mis en fonction de mon User (Les premi√®res commandes)

### Exemple avec mon emplacement de DL
C:\Users\marca\Downloads\elasticsearch-9.0.2-linux-x86_64.tar.gz
C:\Users\marca\Downloads\kibana-9.0.2-linux-x86_64.tar.gz

### D√©placer les dossier : Sur WSL ~/Big-Data-Project$ 
```Bash
cp /mnt/c/Users/marca/Downloads/elasticsearch-9.0.2-linux-x86_64.tar.gz ~/Big-Data-Project/
```
```Bash
cp /mnt/c/Users/marca/Downloads/kibana-9.0.2-linux-x86_64.tar.gz ~/Big-Data-Project/
```

### D√©zipper les dossier : Sur WSL ~/Big-Data-Project$
```Bash
tar -xvzf elasticsearch-9.0.2-linux-x86_64.tar.gz
```
```Bash
tar -xvzf kibana-9.0.2-linux-x86_64.tar.gz
```
### Supprimer les dossier zipp√© : Sur WSL ~/Big-Data-Project$
```Bash
rm -rf ~/Big-Data-Project/elasticsearch-9.0.2-linux-x86_64.tar.gz
```
```Bash
rm -rf ~/Big-Data-Project/kibana-9.0.2-linux-x86_64.tar.gz
```

### Lancer Elasticsearch : Sur WSL ~/Big-Data-Project/elasticsearch-9.0.2$ 
Je te conseil d'ouvrir un nouveau terminal
```Bash
bin/elasticsearch
```
Tu re√ßois un MDP, prend le en note (Exemple du miens : R8=vg8aFslx9fySEIpGY) 
Il faudra modifier le MDP dans le fichier d'indexion.

### Lancer Kibana : Sur WSL ~/Big-Data-Project/kibana-9.0.2$
Je te conseil d'ouvrir un nouveau terminal
```Bash
bin/kibana
```

### R√©cup√©ration d eEnrollment token : Sur WSL ~/Big-Data-Project/elasticsearch-9.0.2$
Enrollment token : 
```Bash
bin/elasticsearch-create-enrollment-token -s kibana
```

### Code de v√©rification : Sur WSL ~/Big-Data-Project/kibana-9.0.2$
Copy the code : 
```Bash
bin/kibana-verification-code
```

### Lancer Manuellement L'indexion : Sur WSL ~/Big-Data-Project/indexing$
```Bash
python3 indexing_kpi.py
```

### Sur Kibana
Username : elastic
Password : Ton MDP (Exemple du miens : R8=vg8aFslx9fySEIpGY) 

Barre de recherche : Visualise library

Create Data View
Nom : kpis
index patern : kpis
Save Data View to Kibana

Create New Visualisation
Lens : permet la cr√©ation des √©l√©ments du Dashboard

### Import du Dashboard
üì• 1Ô∏è‚É£ Acc√©der √† la gestion des objets enregistr√©s
Va dans Kibana.

Clique sur "Stack Management".

Dans la section "Saved Objects", trouve l'option "Import".

üîÑ 2Ô∏è‚É£ Importer le fichier JSON
Clique sur "Import".

S√©lectionne ton fichier JSON export√©.

Valide l'importation et assure-toi que tout est bien pris en compte.

üöÄ 3Ô∏è‚É£ V√©rifier le dashboard
Apr√®s l'importation, va dans "Dashboards".

Ton dashboard import√© devrait appara√Ætre.

Ouvre-le et v√©rifie que les donn√©es sont correctes.