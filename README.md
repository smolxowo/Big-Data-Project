# üöÄ Big Data Project ‚Äì Airflow + PySpark + DataLake

Pipeline de traitement de donn√©es automatis√© et modulaire utilisant Apache Airflow, PySpark, API REST et DataLake.

---

## üìÅ Pr√©requis

- Syst√®me Linux, macOS ou Windows avec WSL  
- Python 3.7+  
- pip  
- git  

Pour Windows, installer WSL : [Installation WSL](https://learn.microsoft.com/fr-fr/windows/wsl/install)

---

## üß± Installation

### 1. Cloner le d√©p√¥t  
`git clone https://github.com/Booboo123478/Big-Data-Project.git`  
`cd Big-Data-Project`

### 2. Installer python3-venv (DANS WSL √† faire une fois)
`sudo apt update`  
`sudo apt install python3-venv`

### 3. Cr√©er un environnement virtuel Python (DANS WSL √† faire une fois)
`python3 -m venv airflow_venv` 

### 4. Activer un environnement virtuel Python (DANS WSL)
`source airflow_venv/bin/activate`

### 5. Installer Apache Airflow (version 2.9.0 avec Celery)  (DANS WSL)
R√©cup√©rer la version de Python :  
`PYTHON_VERSION=$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)`  
Puis installer Airflow avec la contrainte correspondante :  
`pip install "apache-airflow[celery]==2.9.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.0/constraints-$PYTHON_VERSION.txt"`

### 6. Installer les d√©pendances du projet 
`pip install -r requirements.txt`

### 7. D√©finir le dossier des DAGs (Dans le cas o√π tu as d√©j√† d√©finis des DAGs ailleurs)
`export AIRFLOW__CORE__DAGS_FOLDER=/mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project/dags`
# Ou
`nano ~/airflow/airflow.cfg`
# Met en commentaire la ligne suivante
`dags_folder = /home/<TON_USER>/airflow/dags`
# Et rajoute la suivante
`dags_folder = /mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project/dags`

### 8. Lancer Airflow sur le port 8080
`airflow standalone`

### 9.

---

## ‚öôÔ∏è Utilisation

- L‚Äôinterface Airflow est accessible sur http://localhost:8080  
- D√©ployer et g√©rer vos DAGs via l‚Äôinterface  
- Le scheduler orchestre les ex√©cutions selon les plannings
