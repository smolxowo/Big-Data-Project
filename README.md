# üöÄ Big Data Project ‚Äì Airflow + PySpark + DataLake

Pipeline de traitement de donn√©es automatis√© et modulaire utilisant Apache Airflow, PySpark, API REST et DataLake.

---

## üìÅ Pr√©requis

- Syst√®me Linux, macOS ou Windows avec WSL
- Python 3.7+
- pip
- git
- Compte Kaggle avec cl√© API (`kaggle.json`)

Pour Windows, installer WSL : [Installation WSL](https://learn.microsoft.com/fr-fr/windows/wsl/install)

---

## üß± Installation

### 1. Cloner le d√©p√¥t

`git clone https://github.com/Booboo123478/Big-Data-Project.git`  
`cd Big-Data-Project`

### 2. Installer les d√©pendances du projet (Dans le projet)

`pip install -r requirements.txt`

### 3. Chemin classic (Dans WSL) (√† red√©finir √† votre cas)

`cd /mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project`

### 4. Installer python3-venv (DANS WSL √† faire une fois)

`sudo apt update`  
`sudo apt install python3-venv`

### 5. Cr√©er un environnement virtuel Python (DANS WSL √† faire une fois)

`python3 -m venv airflow_venv`

### 6. Activer un environnement virtuel Python (DANS WSL)

`source airflow_venv/bin/activate`

### 7. Installer Apache Airflow (version 2.9.0 avec Celery) (DANS WSL √† faire une fois)

R√©cup√©rer la version de Python :  
`PYTHON_VERSION=$(python3 --version | cut -d " " -f 2 | cut -d "." -f 1-2)`  
Puis installer Airflow avec la contrainte correspondante :  
`pip install "apache-airflow[celery]==2.9.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.0/constraints-$PYTHON_VERSION.txt"`

### 8. D√©finir le dossier des DAGs (Dans le cas o√π tu as d√©j√† d√©finis des DAGs ailleurs)

`export AIRFLOW__CORE__DAGS_FOLDER=/mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project/dags`

#### Ou

`nano ~/airflow/airflow.cfg`

#### Met en commentaire la ligne suivante

`dags_folder = /home/<TON_USER>/airflow/dags`

#### Et rajoute la suivante

`dags_folder = /mnt/c/Users/<TON_USER>/OneDrive/Documents/GitHub/Big-Data-Project/dags`

### 9. Lancer Airflow sur le port 8080

`airflow standalone`

## KAGGLE

### 1. Rentrer cl√© API KAGGLE dans dossier "config"

`Kaggle -> Settings -> API -> Create New Token -> mettre "kaggle.json" dans "config" `

### 2. D√©finir emplacement cl√© API KAGGLE

`export KAGGLE_CONFIG_DIR=/chemin/vers/mon-projet/config`

### 3. T√©l√©charger Data Lake

`python .\download_netflix.py`
`python .\download_tmdb_movies.py`

## Spark

### 1. Installer Java
`sudo apt update`
`sudo apt install openjdk-17-jdk -y`

V√©rification :
`readlink -f $(which java)`

Tu dois obtenir un chemin comme :
`/usr/lib/jvm/java-17-openjdk-amd64/bin/java`

V√©rifie que √ßa correspond bien au script `start_airflow.sh`

---

## ‚öôÔ∏è Utilisation

- L‚Äôinterface Airflow est accessible sur http://localhost:8080
- D√©ployer et g√©rer vos DAGs via l‚Äôinterface
- Le scheduler orchestre les ex√©cutions selon les plannings

## Scripts d√©marrage et arr√™tage Airflow

Rend ex√©cutables les scripts (une seule fois) :
Dans le terminal WSL du projet : `chmod +x start_airflow.sh stop_airflow.sh`

### D√©marrer Airflow

Dans le terminal WSL : `./start_airflow.sh`

Ce script :
- Active automatiquement l‚Äôenvironnement virtuel airflow_venv
- Configure le bon dossier de DAGs pour ce projet (dags/)
- Lance le serveur Airflow (webserver, scheduler, etc.)
Une fois lanc√© :
- Acc√©dez √† l'interface : http://localhost:8080
- Identifiants par d√©faut : admin / admin

### Arr√™ter Airflow proprement

Dans le terminal WSL : `./stop_airflow.sh`

